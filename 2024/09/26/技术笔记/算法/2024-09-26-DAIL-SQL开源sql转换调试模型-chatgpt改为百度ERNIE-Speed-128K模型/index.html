<!DOCTYPE html>
<html lang="zh-CN" color-mode="light">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Del Levin" />
  <!-- Open Graph Description 简短摘要-->
  
  <!-- 用于搜索引擎的文章摘要 -->
  
  <meta name="description" content="望山叹高，登愈艰。入世哀难，伤无泪。" />
  
  
  
  <title>
    
      DAIL-SQL开源sql转换调试模型-chatgpt改为百度ERNIE-Speed-128K模型 
      
      
      |
    
     Del Levin&#39;s Blog
  </title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="/css/iconfont1/iconfont.css" />
  <link rel="stylesheet" href="/css/iconfont2/iconfont.css" />
  <!-- <link rel="stylesheet" href="//at.alicdn.com/t/font_1886449_67xjft27j1l.css" /> -->
  <!-- 代码块风格 -->
  
    
<link rel="stylesheet" href="/css/figcaption/mac-block.css">

  

  <!-- jquery3.3.1 -->
  
    <script defer type="text/javascript" src="/plugins/jquery.min.js"></script>
  

  <!-- fancybox -->
  
    <link href="/plugins/jquery.fancybox.min.css" rel="stylesheet">
    <script defer type="text/javascript" src="/plugins/jquery.fancybox.min.js"></script>
  
  
<script src="/js/fancybox.js"></script>


  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6Z5DFKZBTD"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-6Z5DFKZBTD');
    </script>
  

  

  <script>
    var html = document.documentElement
    const colorMode = localStorage.getItem('color-mode')
    if (colorMode) {
      document.documentElement.setAttribute('color-mode', colorMode)
    }
  </script>
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Del Levin's Blog" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img src="/images/avatar.png" alt="">
      
    </a>
    <div class="nickname"><a href="/">Del Levin</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">Archives</a>
        </li>
      
        <li class="nav-item" data-path="/categories/">
          <a href="/categories/">Categories</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="nav-item" data-path="/album/">
          <a href="/album/">Album</a>
        </li>
      
        <li class="nav-item" data-path="/friends/">
          <a href="/friends/">Friends</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->


  <!-- LaTex Display -->

  
    <script async type="text/javascript" src="/plugins/mathjax/tex-chtml.js"></script>
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    }
  </script>





  <!-- clipboard -->

  
    <script async type="text/javascript" src="/plugins/clipboard.min.js"></script>
  
  
<script src="/js/codeCopy.js"></script>








  

  



  




  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">DAIL-SQL开源sql转换调试模型-chatgpt改为百度ERNIE-Speed-128K模型</div>
      <div class="post-attach">
        <span class="post-num">
          <i class="iconfont icon-edit mr-10" ></i>
          <span id="word-count"></span>
        </span>
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
          2024-11-04 14:39:48
        </span>
        
              <span class="post-categories">
                <i class="iconfont icon-bookmark" title="分类"></i>
                
                <span class="span--category">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" title="技术笔记">
                    <b>#</b> 技术笔记
                  </a>
                </span>
                
              </span>
          
              <span class="post-tags">
                <i class="iconfont icon-tags mr-10" title="标签"></i>
                
                <span class="span--tag mr-8">
                  <a href="/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" title="技术笔记">
                    #技术笔记
                  </a>
                </span>
                
                <span class="span--tag mr-8">
                  <a href="/tags/python/" title="python">
                    #python
                  </a>
                </span>
                
              </span>
          
      </div>
      <script>
        (function() {
          const content = `<p>最近领导要求我带一带他手下的研究生，课题是关于<code>DAIL-SQL</code>这个开源模型调试的。研究生说他已经完成了项目的搭建和gpt模型的调试。接下来要改成百度千帆的模型，却遇到了难题。</p>
<p>其实很简单，根据项目的框架走就可以了。正常部署环境过后（再次会遇到依赖安装不上，版本不匹配等诸多问题，按照报错排错修改即可）。根据readme生成数据模型之后。接下来就该调试ask_llm.py这一步开始了正式的模型修改了。</p>
<p>首先更改一下init方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化百度千帆的api</span></span><br><span class="line">    init_qianfan(args.QIANFAN_ACCESS_KEY, args.QIANFAN_SECRET_KEY, args.model)</span><br></pre></td></tr></table></figure>

<p>然后查看到init方法是自己写的chatgpt的接口解析，我们就需要模仿人家的写法写一个百度千帆的，根据ERNIE_Speed_128K接口模型进行调试更改</p>
<p><a target="_blank" rel="noopener" href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/6ltgkzya5">https://cloud.baidu.com/doc/WENXINWORKSHOP/s/6ltgkzya5</a></p>
<p>之后得出了如下的脚本qianfan.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json.decoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> qianfan</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> utils.enums <span class="keyword">import</span> LLM</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_qianfan</span>(<span class="params">QIANFAN_ACCESS_KEY, QIANFAN_SECRET_KEY,model</span>):</span><br><span class="line">    os.environ[<span class="string">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = QIANFAN_ACCESS_KEY</span><br><span class="line">    os.environ[<span class="string">&quot;QIANFAN_SECRET_KEY&quot;</span>] = QIANFAN_SECRET_KEY</span><br><span class="line"></span><br><span class="line"><span class="comment"># def init_qianfan(QIANFAN_ACCESS_KEY, QIANFAN_SECRET_KEY, model):</span></span><br><span class="line"><span class="comment">#     qianfan.AccessKey(QIANFAN_ACCESS_KEY)</span></span><br><span class="line"><span class="comment">#     qianfan.SecretKey(QIANFAN_SECRET_KEY)</span></span><br><span class="line">    <span class="comment"># os.environ[&quot;QIANFAN_ACCESS_KEY&quot;] = QIANFAN_ACCESS_KEY</span></span><br><span class="line">    <span class="comment"># os.environ[&quot;QIANFAN_SECRET_KEY&quot;] = QIANFAN_SECRET_KEY</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理单轮对话的completion任务</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_completion</span>(<span class="params">model, batch, temperature</span>):</span><br><span class="line">    completion = qianfan.Completion()</span><br><span class="line">    response = completion.do(</span><br><span class="line">        model=model,</span><br><span class="line">        prompt=batch,  <span class="comment"># 这是当前问题</span></span><br><span class="line">        temperature=temperature,</span><br><span class="line">        max_output_tokens=<span class="number">200</span>,  <span class="comment"># 最大输出token数量，根据需要调整</span></span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">        stop=[<span class="string">&quot;;&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 提取response中的结果部分</span></span><br><span class="line">    response_clean = [response[<span class="string">&quot;result&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        response=response_clean,</span><br><span class="line">        prompt_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;prompt_tokens&quot;</span>],</span><br><span class="line">        completion_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;completion_tokens&quot;</span>],</span><br><span class="line">        total_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;total_tokens&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理多轮对话的任务</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_chat</span>(<span class="params">model, messages: <span class="built_in">list</span>, temperature, n</span>):</span><br><span class="line">    chat_completion = qianfan.ChatCompletion()</span><br><span class="line">    response = chat_completion.do(</span><br><span class="line">        model=model,</span><br><span class="line">        messages=messages,  <span class="comment"># messages 是带有历史对话的消息列表</span></span><br><span class="line">        temperature=temperature,</span><br><span class="line">        max_output_tokens=<span class="number">200</span>  <span class="comment"># 最大输出token数量，根据需要调整</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 提取返回的消息内容</span></span><br><span class="line">    response_clean = [response[<span class="string">&quot;result&quot;</span>]]</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        response_clean = response_clean[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        response=response_clean,</span><br><span class="line">        prompt_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;prompt_tokens&quot;</span>],</span><br><span class="line">        completion_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;completion_tokens&quot;</span>],</span><br><span class="line">        total_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;total_tokens&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用对话请i去函数，p判断ask_completion/ask_chat</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_llm</span>(<span class="params">model: <span class="built_in">str</span>, batch: <span class="built_in">list</span>, temperature: <span class="built_in">float</span>, n: <span class="built_in">int</span></span>):</span><br><span class="line">    n_repeat = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> model <span class="keyword">in</span> LLM.TASK_COMPLETIONS:  <span class="comment"># completion任务</span></span><br><span class="line">                <span class="keyword">assert</span> n == <span class="number">1</span></span><br><span class="line">                response = ask_completion(model, batch, temperature)</span><br><span class="line">            <span class="keyword">elif</span> model <span class="keyword">in</span> LLM.TASK_CHAT:  <span class="comment"># chat任务</span></span><br><span class="line">                <span class="keyword">assert</span> <span class="built_in">len</span>(batch) == <span class="number">1</span>, <span class="string">&quot;batch must be 1 in this mode&quot;</span></span><br><span class="line">                messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: batch[<span class="number">0</span>]&#125;]</span><br><span class="line">                response = ask_chat(model, messages, temperature, n)</span><br><span class="line">                response[<span class="string">&#x27;response&#x27;</span>] = [response[<span class="string">&#x27;response&#x27;</span>]]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span> json.decoder.JSONDecodeError:</span><br><span class="line">            n_repeat += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Repeat for the <span class="subst">&#123;n_repeat&#125;</span> times for JSONDecodeError&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            n_repeat += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Repeat for the <span class="subst">&#123;n_repeat&#125;</span> times for exception: <span class="subst">&#123;e&#125;</span>&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<p>接下来你如果运行会发现在ask_llm的时候异常会报错，这是因为异常不匹配，我没有细察qianfan的异常有那些，就直接抛出Exception</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">               res = ask_llm(args.model, batch, args.temperature, args.n)</span><br><span class="line">           <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">               <span class="built_in">print</span>(<span class="string">f&quot;The <span class="subst">&#123;i&#125;</span>-th question has too much tokens! Return \&quot;SELECT\&quot; instead&quot;</span>)</span><br><span class="line">               <span class="comment"># res = &quot;&quot;</span></span><br><span class="line">               res = &#123;<span class="string">&quot;response&quot;</span>: [<span class="string">&quot;&quot;</span>], <span class="string">&quot;total_tokens&quot;</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure>

<p>因为二者模型不一样，还需要对结果集进行数据的搜集更改，我这里没做处理，其实只需要一个简单的正则表达式就可以,在如下更改就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sqls, db_id <span class="keyword">in</span> <span class="built_in">zip</span>(res[<span class="string">&quot;response&quot;</span>], cur_db_ids):</span><br><span class="line">    processed_sqls = []</span><br><span class="line">    <span class="keyword">for</span> sql <span class="keyword">in</span> sqls:</span><br><span class="line">        sql = <span class="string">&quot; &quot;</span>.join(sql.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>).split())</span><br><span class="line">        sql = process_duplication(sql)</span><br><span class="line">        <span class="keyword">if</span> sql.startswith(<span class="string">&quot;SELECT&quot;</span>):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">elif</span> sql.startswith(<span class="string">&quot; &quot;</span>):</span><br><span class="line">            sql = <span class="string">&quot;SELECT&quot;</span> + sql</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sql = <span class="string">&quot;SELECT &quot;</span> + sql</span><br><span class="line">        processed_sqls.append(sql)</span><br></pre></td></tr></table></figure>

<p>由于我没做更改，只得在后续生成的文件中进行数据提取，采用如下方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_sql</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="comment"># 匹配```sql开头 ```结尾</span></span><br><span class="line">    pattern = <span class="string">r&#x27;```sql(.*?)```&#x27;</span></span><br><span class="line">    sql_blocks = re.findall(pattern, content, re.DOTALL)</span><br><span class="line">    sql_statements = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sql <span class="keyword">in</span> sql_blocks:</span><br><span class="line">        cleaned_sql = <span class="string">&quot; &quot;</span>.join(sql.split()) </span><br><span class="line">        sql_statements.append(cleaned_sql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sql_statements</span><br></pre></td></tr></table></figure>



<p>同时还需要再enmus脚本中添加对应的模型来识别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="comment"># openai LLMs</span></span><br><span class="line">    TEXT_DAVINCI_003 = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line">    CODE_DAVINCI_002 = <span class="string">&quot;code-davinci-002&quot;</span></span><br><span class="line">    GPT_35_TURBO = <span class="string">&quot;gpt-3.5-turbo&quot;</span></span><br><span class="line">    GPT_35_TURBO_0613 = <span class="string">&quot;gpt-3.5-turbo-0613&quot;</span></span><br><span class="line">    GPT_35_TURBO_16K = <span class="string">&quot;gpt-3.5-turbo-16k&quot;</span></span><br><span class="line">    GPT_35_TURBO_0301 = <span class="string">&quot;gpt-3.5-turbo-0301&quot;</span></span><br><span class="line">    GPT_4 = <span class="string">&quot;gpt-4&quot;</span></span><br><span class="line">    ERNIE_Speed_128K = <span class="string">&#x27;ERNIE-Speed-128K&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># LLMs that use openai completion api</span></span><br><span class="line">    TASK_COMPLETIONS = [</span><br><span class="line">        TEXT_DAVINCI_003,</span><br><span class="line">        CODE_DAVINCI_002</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LLMs that use openai chat api</span></span><br><span class="line">    TASK_CHAT = [</span><br><span class="line">        GPT_35_TURBO,</span><br><span class="line">        GPT_35_TURBO_0613,</span><br><span class="line">        GPT_35_TURBO_16K,</span><br><span class="line">        GPT_35_TURBO_0301,</span><br><span class="line">        GPT_4,</span><br><span class="line">        ERNIE_Speed_128K</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>脚本ask_llm输入的命令行修改为类似如下的方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--question&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--QIANFAN_ACCESS_KEY&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--QIANFAN_SECRET_KEY&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line"><span class="comment"># parser.add_argument(&quot;--openai_api_key&quot;, type=str)</span></span><br><span class="line"><span class="comment"># parser.add_argument(&quot;--openai_group_id&quot;, type=str, default=&quot;org-ktBefi7n9aK7sZjwc2R9G1Wo&quot;)</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--model&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=[LLM.TEXT_DAVINCI_003, </span><br><span class="line">                                                  LLM.GPT_35_TURBO,</span><br><span class="line">                                                  LLM.GPT_35_TURBO_0613,</span><br><span class="line">                                                  <span class="comment"># LLM.TONG_YI_QIAN_WEN,</span></span><br><span class="line">                                                  LLM.GPT_35_TURBO_16K,</span><br><span class="line">                                                  LLM.GPT_4,</span><br><span class="line">                                                  LLM.ERNIE_Speed_128K],</span><br><span class="line">                    default=LLM.ERNIE_Speed_128K)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--start_index&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--end_index&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000000</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--temperature&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0</span>) <span class="comment"># qianfan  (0, 1.0]</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--mini_index_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--n&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>, <span class="built_in">help</span>=<span class="string">&quot;Size of self-consistent set&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--db_dir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;dataset/spider/database&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>注意千帆的temperature是 (0, 1.0]和gpt的还不一样</p>
<p>至此已经完全修改完毕。</p>
`;
          const text = content.replace(/<\/?[^>]+(>|$)/g, "");
          const wordCount = text.length;
          document.getElementById("word-count").innerText = `${wordCount} 字`;
        })();
      </script>
      <div class="markdown-body">
        <p>最近领导要求我带一带他手下的研究生，课题是关于<code>DAIL-SQL</code>这个开源模型调试的。研究生说他已经完成了项目的搭建和gpt模型的调试。接下来要改成百度千帆的模型，却遇到了难题。</p>
<p>其实很简单，根据项目的框架走就可以了。正常部署环境过后（再次会遇到依赖安装不上，版本不匹配等诸多问题，按照报错排错修改即可）。根据readme生成数据模型之后。接下来就该调试ask_llm.py这一步开始了正式的模型修改了。</p>
<p>首先更改一下init方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化百度千帆的api</span></span><br><span class="line">    init_qianfan(args.QIANFAN_ACCESS_KEY, args.QIANFAN_SECRET_KEY, args.model)</span><br></pre></td></tr></table></figure>

<p>然后查看到init方法是自己写的chatgpt的接口解析，我们就需要模仿人家的写法写一个百度千帆的，根据ERNIE_Speed_128K接口模型进行调试更改</p>
<p><a target="_blank" rel="noopener" href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/6ltgkzya5">https://cloud.baidu.com/doc/WENXINWORKSHOP/s/6ltgkzya5</a></p>
<p>之后得出了如下的脚本qianfan.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json.decoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> qianfan</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> utils.enums <span class="keyword">import</span> LLM</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_qianfan</span>(<span class="params">QIANFAN_ACCESS_KEY, QIANFAN_SECRET_KEY,model</span>):</span><br><span class="line">    os.environ[<span class="string">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = QIANFAN_ACCESS_KEY</span><br><span class="line">    os.environ[<span class="string">&quot;QIANFAN_SECRET_KEY&quot;</span>] = QIANFAN_SECRET_KEY</span><br><span class="line"></span><br><span class="line"><span class="comment"># def init_qianfan(QIANFAN_ACCESS_KEY, QIANFAN_SECRET_KEY, model):</span></span><br><span class="line"><span class="comment">#     qianfan.AccessKey(QIANFAN_ACCESS_KEY)</span></span><br><span class="line"><span class="comment">#     qianfan.SecretKey(QIANFAN_SECRET_KEY)</span></span><br><span class="line">    <span class="comment"># os.environ[&quot;QIANFAN_ACCESS_KEY&quot;] = QIANFAN_ACCESS_KEY</span></span><br><span class="line">    <span class="comment"># os.environ[&quot;QIANFAN_SECRET_KEY&quot;] = QIANFAN_SECRET_KEY</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理单轮对话的completion任务</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_completion</span>(<span class="params">model, batch, temperature</span>):</span><br><span class="line">    completion = qianfan.Completion()</span><br><span class="line">    response = completion.do(</span><br><span class="line">        model=model,</span><br><span class="line">        prompt=batch,  <span class="comment"># 这是当前问题</span></span><br><span class="line">        temperature=temperature,</span><br><span class="line">        max_output_tokens=<span class="number">200</span>,  <span class="comment"># 最大输出token数量，根据需要调整</span></span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">        stop=[<span class="string">&quot;;&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 提取response中的结果部分</span></span><br><span class="line">    response_clean = [response[<span class="string">&quot;result&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        response=response_clean,</span><br><span class="line">        prompt_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;prompt_tokens&quot;</span>],</span><br><span class="line">        completion_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;completion_tokens&quot;</span>],</span><br><span class="line">        total_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;total_tokens&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理多轮对话的任务</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_chat</span>(<span class="params">model, messages: <span class="built_in">list</span>, temperature, n</span>):</span><br><span class="line">    chat_completion = qianfan.ChatCompletion()</span><br><span class="line">    response = chat_completion.do(</span><br><span class="line">        model=model,</span><br><span class="line">        messages=messages,  <span class="comment"># messages 是带有历史对话的消息列表</span></span><br><span class="line">        temperature=temperature,</span><br><span class="line">        max_output_tokens=<span class="number">200</span>  <span class="comment"># 最大输出token数量，根据需要调整</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 提取返回的消息内容</span></span><br><span class="line">    response_clean = [response[<span class="string">&quot;result&quot;</span>]]</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        response_clean = response_clean[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        response=response_clean,</span><br><span class="line">        prompt_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;prompt_tokens&quot;</span>],</span><br><span class="line">        completion_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;completion_tokens&quot;</span>],</span><br><span class="line">        total_tokens=response[<span class="string">&quot;usage&quot;</span>][<span class="string">&quot;total_tokens&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用对话请i去函数，p判断ask_completion/ask_chat</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ask_llm</span>(<span class="params">model: <span class="built_in">str</span>, batch: <span class="built_in">list</span>, temperature: <span class="built_in">float</span>, n: <span class="built_in">int</span></span>):</span><br><span class="line">    n_repeat = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> model <span class="keyword">in</span> LLM.TASK_COMPLETIONS:  <span class="comment"># completion任务</span></span><br><span class="line">                <span class="keyword">assert</span> n == <span class="number">1</span></span><br><span class="line">                response = ask_completion(model, batch, temperature)</span><br><span class="line">            <span class="keyword">elif</span> model <span class="keyword">in</span> LLM.TASK_CHAT:  <span class="comment"># chat任务</span></span><br><span class="line">                <span class="keyword">assert</span> <span class="built_in">len</span>(batch) == <span class="number">1</span>, <span class="string">&quot;batch must be 1 in this mode&quot;</span></span><br><span class="line">                messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: batch[<span class="number">0</span>]&#125;]</span><br><span class="line">                response = ask_chat(model, messages, temperature, n)</span><br><span class="line">                response[<span class="string">&#x27;response&#x27;</span>] = [response[<span class="string">&#x27;response&#x27;</span>]]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span> json.decoder.JSONDecodeError:</span><br><span class="line">            n_repeat += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Repeat for the <span class="subst">&#123;n_repeat&#125;</span> times for JSONDecodeError&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            n_repeat += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Repeat for the <span class="subst">&#123;n_repeat&#125;</span> times for exception: <span class="subst">&#123;e&#125;</span>&quot;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<p>接下来你如果运行会发现在ask_llm的时候异常会报错，这是因为异常不匹配，我没有细察qianfan的异常有那些，就直接抛出Exception</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">               res = ask_llm(args.model, batch, args.temperature, args.n)</span><br><span class="line">           <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">               <span class="built_in">print</span>(<span class="string">f&quot;The <span class="subst">&#123;i&#125;</span>-th question has too much tokens! Return \&quot;SELECT\&quot; instead&quot;</span>)</span><br><span class="line">               <span class="comment"># res = &quot;&quot;</span></span><br><span class="line">               res = &#123;<span class="string">&quot;response&quot;</span>: [<span class="string">&quot;&quot;</span>], <span class="string">&quot;total_tokens&quot;</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure>

<p>因为二者模型不一样，还需要对结果集进行数据的搜集更改，我这里没做处理，其实只需要一个简单的正则表达式就可以,在如下更改就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sqls, db_id <span class="keyword">in</span> <span class="built_in">zip</span>(res[<span class="string">&quot;response&quot;</span>], cur_db_ids):</span><br><span class="line">    processed_sqls = []</span><br><span class="line">    <span class="keyword">for</span> sql <span class="keyword">in</span> sqls:</span><br><span class="line">        sql = <span class="string">&quot; &quot;</span>.join(sql.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>).split())</span><br><span class="line">        sql = process_duplication(sql)</span><br><span class="line">        <span class="keyword">if</span> sql.startswith(<span class="string">&quot;SELECT&quot;</span>):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">elif</span> sql.startswith(<span class="string">&quot; &quot;</span>):</span><br><span class="line">            sql = <span class="string">&quot;SELECT&quot;</span> + sql</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sql = <span class="string">&quot;SELECT &quot;</span> + sql</span><br><span class="line">        processed_sqls.append(sql)</span><br></pre></td></tr></table></figure>

<p>由于我没做更改，只得在后续生成的文件中进行数据提取，采用如下方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_sql</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="comment"># 匹配```sql开头 ```结尾</span></span><br><span class="line">    pattern = <span class="string">r&#x27;```sql(.*?)```&#x27;</span></span><br><span class="line">    sql_blocks = re.findall(pattern, content, re.DOTALL)</span><br><span class="line">    sql_statements = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sql <span class="keyword">in</span> sql_blocks:</span><br><span class="line">        cleaned_sql = <span class="string">&quot; &quot;</span>.join(sql.split()) </span><br><span class="line">        sql_statements.append(cleaned_sql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sql_statements</span><br></pre></td></tr></table></figure>



<p>同时还需要再enmus脚本中添加对应的模型来识别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LLM</span>:</span><br><span class="line">    <span class="comment"># openai LLMs</span></span><br><span class="line">    TEXT_DAVINCI_003 = <span class="string">&quot;text-davinci-003&quot;</span></span><br><span class="line">    CODE_DAVINCI_002 = <span class="string">&quot;code-davinci-002&quot;</span></span><br><span class="line">    GPT_35_TURBO = <span class="string">&quot;gpt-3.5-turbo&quot;</span></span><br><span class="line">    GPT_35_TURBO_0613 = <span class="string">&quot;gpt-3.5-turbo-0613&quot;</span></span><br><span class="line">    GPT_35_TURBO_16K = <span class="string">&quot;gpt-3.5-turbo-16k&quot;</span></span><br><span class="line">    GPT_35_TURBO_0301 = <span class="string">&quot;gpt-3.5-turbo-0301&quot;</span></span><br><span class="line">    GPT_4 = <span class="string">&quot;gpt-4&quot;</span></span><br><span class="line">    ERNIE_Speed_128K = <span class="string">&#x27;ERNIE-Speed-128K&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># LLMs that use openai completion api</span></span><br><span class="line">    TASK_COMPLETIONS = [</span><br><span class="line">        TEXT_DAVINCI_003,</span><br><span class="line">        CODE_DAVINCI_002</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LLMs that use openai chat api</span></span><br><span class="line">    TASK_CHAT = [</span><br><span class="line">        GPT_35_TURBO,</span><br><span class="line">        GPT_35_TURBO_0613,</span><br><span class="line">        GPT_35_TURBO_16K,</span><br><span class="line">        GPT_35_TURBO_0301,</span><br><span class="line">        GPT_4,</span><br><span class="line">        ERNIE_Speed_128K</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>脚本ask_llm输入的命令行修改为类似如下的方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--question&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--QIANFAN_ACCESS_KEY&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--QIANFAN_SECRET_KEY&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line"><span class="comment"># parser.add_argument(&quot;--openai_api_key&quot;, type=str)</span></span><br><span class="line"><span class="comment"># parser.add_argument(&quot;--openai_group_id&quot;, type=str, default=&quot;org-ktBefi7n9aK7sZjwc2R9G1Wo&quot;)</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--model&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=[LLM.TEXT_DAVINCI_003, </span><br><span class="line">                                                  LLM.GPT_35_TURBO,</span><br><span class="line">                                                  LLM.GPT_35_TURBO_0613,</span><br><span class="line">                                                  <span class="comment"># LLM.TONG_YI_QIAN_WEN,</span></span><br><span class="line">                                                  LLM.GPT_35_TURBO_16K,</span><br><span class="line">                                                  LLM.GPT_4,</span><br><span class="line">                                                  LLM.ERNIE_Speed_128K],</span><br><span class="line">                    default=LLM.ERNIE_Speed_128K)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--start_index&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--end_index&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000000</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--temperature&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0</span>) <span class="comment"># qianfan  (0, 1.0]</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--mini_index_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--n&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>, <span class="built_in">help</span>=<span class="string">&quot;Size of self-consistent set&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--db_dir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;dataset/spider/database&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>注意千帆的temperature是 (0, 1.0]和gpt的还不一样</p>
<p>至此已经完全修改完毕。</p>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2024/09/26/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E7%BD%91%E7%BB%9C/2024-09-26-Windows_%E4%B8%8A%E8%BF%90%E8%A1%8C_Stanford_CoreNLP_%E6%9C%8D%E5%8A%A1/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>上一页</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
              2024-11-04 14:39:48
            </span>
            
                  <span class="post-categories">
                    <i class="iconfont icon-bookmark" title="分类"></i>
                    
                    <span class="span--category">
                      <a href="/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" title="技术笔记">
                        <b>#</b> 技术笔记
                      </a>
                    </span>
                    
                  </span>
              
                  <span class="post-tags">
                    <i class="iconfont icon-tags mr-10" title="标签"></i>
                    
                    <span class="span--tag mr-8">
                      <a href="/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" title="技术笔记">
                        #技术笔记
                      </a>
                    </span>
                    
                    <span class="span--tag mr-8">
                      <a href="/tags/python/" title="python">
                        #python
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/2024/09/26/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/java/2024-09-26-java%E9%A1%B9%E7%9B%AE%E5%BC%95%E5%85%A5resources%E4%B8%8B%E9%9D%A2%E7%9A%84%E6%96%87%E4%BB%B6/" target="_self">
                <span>下一页</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    

  </div>


        
  
<div class="footer">
  <div class="social">
    <ul>
      
        <li>
          
              <a title="github" target="_blank" rel="noopener" href="https://github.com/dellevin/dellevin.github.io">
                <i class="iconfont icon-github"></i>
              </a>
              
        </li>
        
        <li>
          
            <a title="email" href="mailto:dellevin99@gmail.com">
              <i class="iconfont icon-envelope"></i>
            </a>
            
        </li>
        
        <li>
          
              <a title="telegram" target="_blank" rel="noopener" href="https://t.me/levin092">
                <i class="iconfont icon-telegram"></i>
              </a>
              
        </li>
        
        <li>
          
              <a title="weibo" target="_blank" rel="noopener" href="https://weibo.com/u/6094785105">
                <i class="iconfont icon-weibo"></i>
              </a>
              
        </li>
        
        <li>
          
              <a title="rss" href="/atom.xml">
                <i class="iconfont icon-rss"></i>
              </a>
              
        </li>
        
    </ul>
  </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/dellevin">Copyright © 2020 ~ 2025 Del Levin</a>
        
    </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
        
    </div>
  
</div>

      </div>

      <div class="tools-bar">
        <div class="back-to-top tools-bar-item hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



        
  <div class="search-icon tools-bar-item" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>首次搜索，正在载入索引文件，请稍后……<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>没有找到内容，请尝试更换检索词。<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        },
        error: function(xhr, status, error) {
          $resultContent.innerHTML = ""
          if (xhr.status === 404) {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>未找到search.xml文件，具体请参考：<a href='https://github.com/zchengsite/hexo-theme-oranges#configuration' target='_black'>configuration</a><span></ul>";
          } else {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>请求失败，尝试重新刷新页面或稍后重试。<span></ul>";
          }
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




        
  <div class="tools-bar-item theme-icon" id="switch-color-scheme">
    <a href="javascript: void(0)">
      <i id="theme-icon" class="iconfont icon-moon"></i>
    </a>
  </div>

  
<script src="/js/colorscheme.js"></script>





        
  
    <div class="share-icon tools-bar-item">
      <a href="javascript: void(0)" id="share-icon">
        <i class="iconfont iconshare"></i>
      </a>
      <div class="share-content hidden">
        
          <a class="share-item" href="https://twitter.com/intent/tweet?text=' + DAIL-SQL%E5%BC%80%E6%BA%90sql%E8%BD%AC%E6%8D%A2%E8%B0%83%E8%AF%95%E6%A8%A1%E5%9E%8B-chatgpt%E6%94%B9%E4%B8%BA%E7%99%BE%E5%BA%A6ERNIE-Speed-128K%E6%A8%A1%E5%9E%8B + '&url=' + https%3A%2F%2Fwww.ittoolman.top%2F2024%2F09%2F26%2F%25E6%258A%2580%25E6%259C%25AF%25E7%25AC%2594%25E8%25AE%25B0%2F%25E7%25AE%2597%25E6%25B3%2595%2F2024-09-26-DAIL-SQL%25E5%25BC%2580%25E6%25BA%2590sql%25E8%25BD%25AC%25E6%258D%25A2%25E8%25B0%2583%25E8%25AF%2595%25E6%25A8%25A1%25E5%259E%258B-chatgpt%25E6%2594%25B9%25E4%25B8%25BA%25E7%2599%25BE%25E5%25BA%25A6ERNIE-Speed-128K%25E6%25A8%25A1%25E5%259E%258B%2F + '" target="_blank" title="Twitter">
            <i class="iconfont icon-twitter"></i>
          </a>
        
        
          <a class="share-item" href="https://www.facebook.com/sharer.php?u=https://www.ittoolman.top/2024/09/26/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/2024-09-26-DAIL-SQL%E5%BC%80%E6%BA%90sql%E8%BD%AC%E6%8D%A2%E8%B0%83%E8%AF%95%E6%A8%A1%E5%9E%8B-chatgpt%E6%94%B9%E4%B8%BA%E7%99%BE%E5%BA%A6ERNIE-Speed-128K%E6%A8%A1%E5%9E%8B/" target="_blank" title="Facebook">
            <i class="iconfont icon-facebooksquare"></i>
          </a>
        
      </div>
    </div>
  
  
<script src="/js/shares.js"></script>



      </div>
    </div>
  </body>
</html>
